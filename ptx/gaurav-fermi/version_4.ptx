//
// Generated by LLVM NVPTX Back-End
//

.version 3.1
.target sm_20
.address_size 64

	// .globl	kernel_S_version_4_s0_y_yo___block_id_y
                                        // @kernel_S_version_4_s0_y_yo___block_id_y
.visible .entry kernel_S_version_4_s0_y_yo___block_id_y(
	.param .u32 kernel_S_version_4_s0_y_yo___block_id_y_param_0,
	.param .u32 kernel_S_version_4_s0_y_yo___block_id_y_param_1,
	.param .u32 kernel_S_version_4_s0_y_yo___block_id_y_param_2,
	.param .u64 kernel_S_version_4_s0_y_yo___block_id_y_param_3,
	.param .u64 kernel_S_version_4_s0_y_yo___block_id_y_param_4
)
{
	.reg .pred 	%p<9>;
	.reg .f32 	%f<13>;
	.reg .s32 	%r<21>;
	.reg .s64 	%rl<132>;

// BB#0:                                // %entry
	ld.param.u64 	%rl21, [kernel_S_version_4_s0_y_yo___block_id_y_param_3];
	ld.param.u32 	%r4, [kernel_S_version_4_s0_y_yo___block_id_y_param_2];
	ld.param.u32 	%r3, [kernel_S_version_4_s0_y_yo___block_id_y_param_1];
	ld.param.u32 	%r2, [kernel_S_version_4_s0_y_yo___block_id_y_param_0];
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r7, %tid.y;
	mov.u32 	%r1, %tid.x;
	ld.param.u64 	%rl22, [kernel_S_version_4_s0_y_yo___block_id_y_param_4];
	mul.lo.s32 	%r8, %r7, 6;
	setp.lt.s32	%p3, %r8, 26;
	cvt.s64.s32	%rl23, %r8;
	selp.b64	%rl1, %rl23, 26, %p3;
	mul.wide.s32 	%rl2, %r5, 32;
	cvt.s64.s32	%rl3, %r1;
	mul.wide.s32 	%rl24, %r6, 32;
	add.s64 	%rl4, %rl3, %rl24;
	add.s64 	%rl25, %rl1, %rl2;
	shl.b64 	%rl26, %rl25, 12;
	add.s64 	%rl27, %rl26, %rl4;
	shl.b64 	%rl28, %rl27, 2;
	add.s64 	%rl29, %rl22, %rl28;
	ld.f32 	%f1, [%rl29];
	shl.b64 	%rl5, %rl1, 5;
	add.s64 	%rl30, %rl5, %rl3;
	shl.b64 	%rl31, %rl30, 2;
	st.shared.f32 	[%rl31], %f1;
	or.b64  	%rl32, %rl1, 1;
	add.s64 	%rl33, %rl32, %rl2;
	shl.b64 	%rl34, %rl33, 12;
	add.s64 	%rl35, %rl34, %rl4;
	shl.b64 	%rl36, %rl35, 2;
	add.s64 	%rl37, %rl22, %rl36;
	ld.f32 	%f2, [%rl37];
	shl.b64 	%rl6, %rl32, 5;
	add.s64 	%rl38, %rl6, %rl3;
	shl.b64 	%rl39, %rl38, 2;
	st.shared.f32 	[%rl39], %f2;
	add.s64 	%rl7, %rl1, 2;
	add.s64 	%rl40, %rl7, %rl2;
	shl.b64 	%rl41, %rl40, 12;
	add.s64 	%rl42, %rl41, %rl4;
	shl.b64 	%rl43, %rl42, 2;
	add.s64 	%rl44, %rl22, %rl43;
	ld.f32 	%f3, [%rl44];
	shl.b64 	%rl8, %rl7, 5;
	add.s64 	%rl45, %rl8, %rl3;
	shl.b64 	%rl46, %rl45, 2;
	st.shared.f32 	[%rl46], %f3;
	add.s64 	%rl9, %rl1, 3;
	add.s64 	%rl47, %rl9, %rl2;
	shl.b64 	%rl48, %rl47, 12;
	add.s64 	%rl49, %rl48, %rl4;
	shl.b64 	%rl50, %rl49, 2;
	add.s64 	%rl51, %rl22, %rl50;
	ld.f32 	%f4, [%rl51];
	shl.b64 	%rl10, %rl9, 5;
	add.s64 	%rl52, %rl10, %rl3;
	shl.b64 	%rl53, %rl52, 2;
	st.shared.f32 	[%rl53], %f4;
	add.s64 	%rl11, %rl1, 4;
	add.s64 	%rl54, %rl11, %rl2;
	shl.b64 	%rl55, %rl54, 12;
	add.s64 	%rl56, %rl55, %rl4;
	shl.b64 	%rl57, %rl56, 2;
	add.s64 	%rl58, %rl22, %rl57;
	ld.f32 	%f5, [%rl58];
	shl.b64 	%rl12, %rl11, 5;
	add.s64 	%rl59, %rl12, %rl3;
	shl.b64 	%rl60, %rl59, 2;
	st.shared.f32 	[%rl60], %f5;
	add.s64 	%rl13, %rl1, 5;
	add.s64 	%rl61, %rl13, %rl2;
	shl.b64 	%rl62, %rl61, 12;
	add.s64 	%rl63, %rl62, %rl4;
	shl.b64 	%rl64, %rl63, 2;
	add.s64 	%rl65, %rl22, %rl64;
	ld.f32 	%f6, [%rl65];
	shl.b64 	%rl14, %rl13, 5;
	add.s64 	%rl66, %rl14, %rl3;
	shl.b64 	%rl67, %rl66, 2;
	st.shared.f32 	[%rl67], %f6;
	bar.sync 	0;
	setp.gt.s32	%p4, %r7, 0;
	@%p4 bra 	BB0_1;
	bra.uni 	BB0_2;
BB0_1:
	mov.pred 	%p8, 0;
	bra.uni 	BB0_3;
BB0_2:                                  // %for SI_version_4.s1.rxi.x$r.preheader
	mul.wide.s32 	%rl15, %r1, 128;
	mov.u64 	%rl130, 4;
BB0_5:                                  // %for SI_version_4.s1.rxi.x$r
                                        // =>This Inner Loop Header: Depth=1
	add.s64 	%rl69, %rl15, %rl130;
	mov.u32 	%r9, 0;
	st.shared.u32 	[%rl69], %r9;
	add.s64 	%rl130, %rl130, 4;
	setp.eq.s64	%p6, %rl130, 128;
	@%p6 bra 	BB0_6;
	bra.uni 	BB0_5;
BB0_6:
	mov.pred 	%p8, -1;
BB0_3:                                  // %after_bb
	bar.sync 	0;
	@!%p8 bra 	BB0_7;
	bra.uni 	BB0_4;
BB0_4:                                  // %for SI_version_4.s2.ryi.x$r.preheader
	mul.wide.s32 	%rl16, %r1, 4;
	mov.u64 	%rl131, 128;
BB0_8:                                  // %for SI_version_4.s2.ryi.x$r
                                        // =>This Inner Loop Header: Depth=1
	add.s64 	%rl71, %rl16, %rl131;
	mov.u32 	%r10, 0;
	st.shared.u32 	[%rl71], %r10;
	add.s64 	%rl131, %rl131, 128;
	setp.eq.s64	%p7, %rl131, 4096;
	@%p7 bra 	BB0_7;
	bra.uni 	BB0_8;
BB0_7:                                  // %after_bb3
	bar.sync 	0;
	cvt.s64.s32	%rl72, %r4;
	cvt.s64.s32	%rl73, %r3;
	sub.s64 	%rl74, %rl2, %rl73;
	add.s64 	%rl75, %rl74, %rl1;
	cvt.s64.s32	%rl76, %r2;
	sub.s64 	%rl77, %rl4, %rl76;
	cvt.u32.u64	%r11, %rl1;
	shr.s32 	%r12, %r11, 5;
	mul.wide.s32 	%rl78, %r12, 1024;
	and.b64  	%rl79, %rl5, 960;
	add.s64 	%rl80, %rl79, %rl3;
	add.s64 	%rl81, %rl80, %rl78;
	shl.b64 	%rl82, %rl81, 2;
	ld.shared.f32 	%f7, [%rl82];
	mad.lo.s64 	%rl83, %rl75, %rl72, %rl77;
	shl.b64 	%rl84, %rl83, 2;
	add.s64 	%rl85, %rl21, %rl84;
	st.f32 	[%rl85], %f7;
	and.b64  	%rl86, %rl6, 992;
	add.s64 	%rl87, %rl86, %rl3;
	add.s64 	%rl88, %rl87, %rl78;
	shl.b64 	%rl89, %rl88, 2;
	ld.shared.f32 	%f8, [%rl89];
	add.s64 	%rl90, %rl75, 1;
	mad.lo.s64 	%rl91, %rl90, %rl72, %rl77;
	shl.b64 	%rl92, %rl91, 2;
	add.s64 	%rl93, %rl21, %rl92;
	st.f32 	[%rl93], %f8;
	cvt.u32.u64	%r13, %rl7;
	shr.s32 	%r14, %r13, 5;
	mul.wide.s32 	%rl94, %r14, 1024;
	and.b64  	%rl95, %rl8, 960;
	add.s64 	%rl96, %rl95, %rl3;
	add.s64 	%rl97, %rl96, %rl94;
	shl.b64 	%rl98, %rl97, 2;
	ld.shared.f32 	%f9, [%rl98];
	add.s64 	%rl99, %rl75, 2;
	mad.lo.s64 	%rl100, %rl99, %rl72, %rl77;
	shl.b64 	%rl101, %rl100, 2;
	add.s64 	%rl102, %rl21, %rl101;
	st.f32 	[%rl102], %f9;
	cvt.u32.u64	%r15, %rl9;
	shr.s32 	%r16, %r15, 5;
	mul.wide.s32 	%rl103, %r16, 1024;
	and.b64  	%rl104, %rl10, 992;
	add.s64 	%rl105, %rl104, %rl3;
	add.s64 	%rl106, %rl105, %rl103;
	shl.b64 	%rl107, %rl106, 2;
	ld.shared.f32 	%f10, [%rl107];
	add.s64 	%rl108, %rl75, 3;
	mad.lo.s64 	%rl109, %rl108, %rl72, %rl77;
	shl.b64 	%rl110, %rl109, 2;
	add.s64 	%rl111, %rl21, %rl110;
	st.f32 	[%rl111], %f10;
	cvt.u32.u64	%r17, %rl11;
	shr.s32 	%r18, %r17, 5;
	mul.wide.s32 	%rl112, %r18, 1024;
	and.b64  	%rl113, %rl12, 960;
	add.s64 	%rl114, %rl113, %rl3;
	add.s64 	%rl115, %rl114, %rl112;
	shl.b64 	%rl116, %rl115, 2;
	ld.shared.f32 	%f11, [%rl116];
	add.s64 	%rl117, %rl75, 4;
	mad.lo.s64 	%rl118, %rl117, %rl72, %rl77;
	shl.b64 	%rl119, %rl118, 2;
	add.s64 	%rl120, %rl21, %rl119;
	st.f32 	[%rl120], %f11;
	cvt.u32.u64	%r19, %rl13;
	shr.s32 	%r20, %r19, 5;
	mul.wide.s32 	%rl121, %r20, 1024;
	and.b64  	%rl122, %rl14, 992;
	add.s64 	%rl123, %rl122, %rl3;
	add.s64 	%rl124, %rl123, %rl121;
	shl.b64 	%rl125, %rl124, 2;
	ld.shared.f32 	%f12, [%rl125];
	add.s64 	%rl126, %rl75, 5;
	mad.lo.s64 	%rl127, %rl126, %rl72, %rl77;
	shl.b64 	%rl128, %rl127, 2;
	add.s64 	%rl129, %rl21, %rl128;
	st.f32 	[%rl129], %f12;
	ret;
}
