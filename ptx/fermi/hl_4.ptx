//
// Generated by LLVM NVPTX Back-End
//

.version 3.1
.target sm_20, texmode_independent
.address_size 64

	// .globl	kernel_S_version_4_s0_y_yo___block_id_y
                                        // @kernel_S_version_4_s0_y_yo___block_id_y
.entry kernel_S_version_4_s0_y_yo___block_id_y(
	.param .u32 kernel_S_version_4_s0_y_yo___block_id_y_param_0,
	.param .u32 kernel_S_version_4_s0_y_yo___block_id_y_param_1,
	.param .u32 kernel_S_version_4_s0_y_yo___block_id_y_param_2,
	.param .u64 .ptr .align 1 kernel_S_version_4_s0_y_yo___block_id_y_param_3,
	.param .u64 .ptr .align 1 kernel_S_version_4_s0_y_yo___block_id_y_param_4
)
{
	.reg .pred %p<396>;
	.reg .s16 %rc<396>;
	.reg .s16 %rs<396>;
	.reg .s32 %r<396>;
	.reg .s64 %rl<396>;
	.reg .f32 %f<396>;
	.reg .f64 %fl<396>;

// BB#0:                                // %entry
	ld.param.u32 	%r0, [kernel_S_version_4_s0_y_yo___block_id_y_param_0];
	mov.u32 	%r5, %ctaid.y;
	ld.param.u32 	%r1, [kernel_S_version_4_s0_y_yo___block_id_y_param_1];
	mov.u32 	%r6, %ctaid.x;
	ld.param.u32 	%r2, [kernel_S_version_4_s0_y_yo___block_id_y_param_2];
	mov.u32 	%r4, %tid.y;
	ld.param.u64 	%rl0, [kernel_S_version_4_s0_y_yo___block_id_y_param_3];
	mov.u32 	%r3, %tid.x;
	mul.lo.s32 	%r7, %r4, 6;
	ld.param.u64 	%rl6, [kernel_S_version_4_s0_y_yo___block_id_y_param_4];
	setp.lt.s32 	%p0, %r7, 26;
	cvt.s64.s32 	%rl1, %r7;
	selp.b64 	%rl3, %rl1, 26, %p0;
	mul.wide.s32 	%rl9, %r5, 32;
	cvt.s64.s32 	%rl1, %r3;
	mul.wide.s32 	%rl2, %r6, 32;
	add.s64 	%rl10, %rl1, %rl2;
	add.s64 	%rl2, %rl3, %rl9;
	shl.b64 	%rl2, %rl2, 12;
	add.s64 	%rl2, %rl2, %rl10;
	shl.b64 	%rl2, %rl2, 2;
	add.s64 	%rl2, %rl6, %rl2;
	ld.f32 	%f0, [%rl2];
	shl.b64 	%rl12, %rl3, 5;
	add.s64 	%rl2, %rl12, %rl1;
	shl.b64 	%rl2, %rl2, 2;
	st.shared.f32 	[%rl2], %f0;
	or.b64  	%rl2, %rl3, 1;
	add.s64 	%rl4, %rl2, %rl9;
	shl.b64 	%rl4, %rl4, 12;
	add.s64 	%rl4, %rl4, %rl10;
	shl.b64 	%rl4, %rl4, 2;
	add.s64 	%rl4, %rl6, %rl4;
	ld.f32 	%f0, [%rl4];
	shl.b64 	%rl14, %rl2, 5;
	add.s64 	%rl2, %rl14, %rl1;
	shl.b64 	%rl2, %rl2, 2;
	st.shared.f32 	[%rl2], %f0;
	add.s64 	%rl7, %rl3, 2;
	add.s64 	%rl2, %rl7, %rl9;
	shl.b64 	%rl2, %rl2, 12;
	add.s64 	%rl2, %rl2, %rl10;
	shl.b64 	%rl2, %rl2, 2;
	add.s64 	%rl2, %rl6, %rl2;
	ld.f32 	%f0, [%rl2];
	shl.b64 	%rl13, %rl7, 5;
	add.s64 	%rl2, %rl13, %rl1;
	shl.b64 	%rl2, %rl2, 2;
	st.shared.f32 	[%rl2], %f0;
	add.s64 	%rl5, %rl3, 3;
	add.s64 	%rl2, %rl5, %rl9;
	shl.b64 	%rl2, %rl2, 12;
	add.s64 	%rl2, %rl2, %rl10;
	shl.b64 	%rl2, %rl2, 2;
	add.s64 	%rl2, %rl6, %rl2;
	ld.f32 	%f0, [%rl2];
	shl.b64 	%rl11, %rl5, 5;
	add.s64 	%rl2, %rl11, %rl1;
	shl.b64 	%rl2, %rl2, 2;
	st.shared.f32 	[%rl2], %f0;
	add.s64 	%rl4, %rl3, 4;
	add.s64 	%rl2, %rl4, %rl9;
	shl.b64 	%rl2, %rl2, 12;
	add.s64 	%rl2, %rl2, %rl10;
	shl.b64 	%rl2, %rl2, 2;
	add.s64 	%rl2, %rl6, %rl2;
	ld.f32 	%f0, [%rl2];
	shl.b64 	%rl8, %rl4, 5;
	add.s64 	%rl2, %rl8, %rl1;
	shl.b64 	%rl2, %rl2, 2;
	st.shared.f32 	[%rl2], %f0;
	add.s64 	%rl2, %rl3, 5;
	add.s64 	%rl15, %rl2, %rl9;
	shl.b64 	%rl15, %rl15, 12;
	add.s64 	%rl15, %rl15, %rl10;
	shl.b64 	%rl15, %rl15, 2;
	add.s64 	%rl6, %rl6, %rl15;
	ld.f32 	%f0, [%rl6];
	shl.b64 	%rl6, %rl2, 5;
	add.s64 	%rl15, %rl6, %rl1;
	shl.b64 	%rl15, %rl15, 2;
	st.shared.f32 	[%rl15], %f0;
	bar.sync 	0;
	setp.gt.s32 	%p0, %r4, 0;
	@%p0 bra 	BB0_5;
	bra.uni 	BB0_1;
BB0_5:                                  // %after_bb.thread
	bar.sync 	0;
	bra.uni 	BB0_6;
BB0_1:                                  // %for SI_version_4.s1.rxi.x$r.preheader
	mul.wide.s32 	%rl15, %r3, 128;
	or.b64  	%rl15, %rl15, 4;
	mov.u32 	%r5, 31;
	mov.u32 	%r4, 0;
BB0_2:                                  // %for SI_version_4.s1.rxi.x$r
                                        // =>This Inner Loop Header: Depth=1
	add.s64 	%rl16, %rl15, 4;
	add.s32 	%r5, %r5, -1;
	setp.eq.s32 	%p0, %r5, 0;
	st.shared.u32 	[%rl15], %r4;
	mov.u64 	%rl15, %rl16;
	@%p0 bra 	BB0_3;
	bra.uni 	BB0_2;
BB0_3:                                  // %for SI_version_4.s2.ryi.x$r.preheader
	bar.sync 	0;
	mul.wide.s32 	%rl15, %r3, 4;
	add.s64 	%rl15, %rl15, 128;
	mov.u32 	%r3, 31;
BB0_4:                                  // %for SI_version_4.s2.ryi.x$r
                                        // =>This Inner Loop Header: Depth=1
	add.s64 	%rl16, %rl15, 128;
	add.s32 	%r3, %r3, -1;
	setp.eq.s32 	%p0, %r3, 0;
	st.shared.u32 	[%rl15], %r4;
	mov.u64 	%rl15, %rl16;
	@%p0 bra 	BB0_6;
	bra.uni 	BB0_4;
BB0_6:                                  // %after_bb3
	bar.sync 	0;
	cvt.s64.s32 	%rl15, %r2;
	cvt.s64.s32 	%rl16, %r1;
	sub.s64 	%rl9, %rl9, %rl16;
	add.s64 	%rl9, %rl9, %rl3;
	cvt.s64.s32 	%rl16, %r0;
	sub.s64 	%rl10, %rl10, %rl16;
	cvt.u32.u64 	%r0, %rl3;
	shr.s32 	%r0, %r0, 5;
	mul.wide.s32 	%rl3, %r0, 1024;
	and.b64  	%rl12, %rl12, 960;
	add.s64 	%rl12, %rl12, %rl1;
	add.s64 	%rl12, %rl12, %rl3;
	shl.b64 	%rl12, %rl12, 2;
	ld.shared.f32 	%f0, [%rl12];
	mad.lo.s64 	%rl12, %rl9, %rl15, %rl10;
	shl.b64 	%rl12, %rl12, 2;
	add.s64 	%rl12, %rl0, %rl12;
	st.f32 	[%rl12], %f0;
	and.b64  	%rl12, %rl14, 992;
	add.s64 	%rl12, %rl12, %rl1;
	add.s64 	%rl3, %rl12, %rl3;
	shl.b64 	%rl3, %rl3, 2;
	ld.shared.f32 	%f0, [%rl3];
	add.s64 	%rl3, %rl9, 1;
	mad.lo.s64 	%rl3, %rl3, %rl15, %rl10;
	shl.b64 	%rl3, %rl3, 2;
	add.s64 	%rl3, %rl0, %rl3;
	st.f32 	[%rl3], %f0;
	cvt.u32.u64 	%r0, %rl7;
	shr.s32 	%r0, %r0, 5;
	mul.wide.s32 	%rl3, %r0, 1024;
	and.b64  	%rl7, %rl13, 960;
	add.s64 	%rl7, %rl7, %rl1;
	add.s64 	%rl3, %rl7, %rl3;
	shl.b64 	%rl3, %rl3, 2;
	ld.shared.f32 	%f0, [%rl3];
	add.s64 	%rl3, %rl9, 2;
	mad.lo.s64 	%rl3, %rl3, %rl15, %rl10;
	shl.b64 	%rl3, %rl3, 2;
	add.s64 	%rl3, %rl0, %rl3;
	st.f32 	[%rl3], %f0;
	cvt.u32.u64 	%r0, %rl5;
	shr.s32 	%r0, %r0, 5;
	mul.wide.s32 	%rl3, %r0, 1024;
	and.b64  	%rl5, %rl11, 992;
	add.s64 	%rl5, %rl5, %rl1;
	add.s64 	%rl3, %rl5, %rl3;
	shl.b64 	%rl3, %rl3, 2;
	ld.shared.f32 	%f0, [%rl3];
	add.s64 	%rl3, %rl9, 3;
	mad.lo.s64 	%rl3, %rl3, %rl15, %rl10;
	shl.b64 	%rl3, %rl3, 2;
	add.s64 	%rl3, %rl0, %rl3;
	st.f32 	[%rl3], %f0;
	cvt.u32.u64 	%r0, %rl4;
	shr.s32 	%r0, %r0, 5;
	mul.wide.s32 	%rl3, %r0, 1024;
	and.b64  	%rl4, %rl8, 960;
	add.s64 	%rl4, %rl4, %rl1;
	add.s64 	%rl3, %rl4, %rl3;
	shl.b64 	%rl3, %rl3, 2;
	ld.shared.f32 	%f0, [%rl3];
	add.s64 	%rl3, %rl9, 4;
	mad.lo.s64 	%rl3, %rl3, %rl15, %rl10;
	shl.b64 	%rl3, %rl3, 2;
	add.s64 	%rl3, %rl0, %rl3;
	st.f32 	[%rl3], %f0;
	cvt.u32.u64 	%r0, %rl2;
	shr.s32 	%r0, %r0, 5;
	mul.wide.s32 	%rl2, %r0, 1024;
	and.b64  	%rl3, %rl6, 992;
	add.s64 	%rl1, %rl3, %rl1;
	add.s64 	%rl1, %rl1, %rl2;
	shl.b64 	%rl1, %rl1, 2;
	ld.shared.f32 	%f0, [%rl1];
	add.s64 	%rl1, %rl9, 5;
	mad.lo.s64 	%rl1, %rl1, %rl15, %rl10;
	shl.b64 	%rl1, %rl1, 2;
	add.s64 	%rl0, %rl0, %rl1;
	st.f32 	[%rl0], %f0;
	ret;
}
