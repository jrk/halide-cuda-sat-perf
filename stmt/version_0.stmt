Injecting realization of SI_version_0
Inlining Input
Injecting tracing...
Injecting profiling...
Adding checks for parameters
Computing bounds of each function's value
Adding checks for images
Performing computation bounds inference...
Performing sliding window optimization...
Performing allocation bounds inference...
Uniquifying variable names...
Performing storage folding optimization...
Injecting debug_to_file calls...
Simplifying...
Dynamically skipping stages...
Performing storage flattening...
Injecting host <-> dev buffer copies...
Injecting per-block gpu synchronization...
Removing code that depends on undef values...
Simplifying...
Unrolling...
Simplifying...
Vectorizing...
Simplifying...
Specializing clamped ramps...
Detecting vector interleavings...
Injecting early frees...
Injecting device frees...
Simplifying...
Simplified:
if (S_version_0.host_and_dev_are_null) {
  rewrite_buffer(S_version_0.buffer, 4, 0, 4096, 1, 0, 4096, 4096)
}
if (i0.host_and_dev_are_null) {
  rewrite_buffer(i0.buffer, 4, 0, 4096, 1, 0, 4096, 4096)
}
if (!(S_version_0.host_and_dev_are_null || i0.host_and_dev_are_null)) {
  assert((S_version_0.elem_size == 4), "Output buffer S_version_0 has type float32, but elem_size of the buffer_t passed in is %d instead of 4", S_version_0.elem_size)
  assert((i0.elem_size == 4), "Input buffer i0 has type float32, but elem_size of the buffer_t passed in is %d instead of 4", i0.elem_size)
  assert((S_version_0.min.0 <= 0), "Output buffer S_version_0 is accessed at %d, which is before the min (%d) in dimension 0", 0, S_version_0.min.0)
  assert(((4096 - S_version_0.extent.0) <= S_version_0.min.0), "Output buffer S_version_0 is accessed at %d, which is beyond the max (%d) in dimension 0", 4095, ((S_version_0.min.0 + S_version_0.extent.0) + -1))
  assert((S_version_0.min.1 <= 0), "Output buffer S_version_0 is accessed at %d, which is before the min (%d) in dimension 1", 0, S_version_0.min.1)
  assert(((4096 - S_version_0.extent.1) <= S_version_0.min.1), "Output buffer S_version_0 is accessed at %d, which is beyond the max (%d) in dimension 1", 4095, ((S_version_0.min.1 + S_version_0.extent.1) + -1))
  assert((i0.min.0 <= 0), "Input buffer i0 is accessed at %d, which is before the min (%d) in dimension 0", 0, i0.min.0)
  assert(((4096 - i0.extent.0) <= i0.min.0), "Input buffer i0 is accessed at %d, which is beyond the max (%d) in dimension 0", 4095, ((i0.min.0 + i0.extent.0) + -1))
  assert((i0.min.1 <= 0), "Input buffer i0 is accessed at %d, which is before the min (%d) in dimension 1", 0, i0.min.1)
  assert(((4096 - i0.extent.1) <= i0.min.1), "Input buffer i0 is accessed at %d, which is beyond the max (%d) in dimension 1", 4095, ((i0.min.1 + i0.extent.1) + -1))
  assert((S_version_0.stride.0 == 1), "Static constraint violated: S_version_0.stride.0 == 1")
  assert((i0.stride.0 == 1), "Static constraint violated: i0.stride.0 == 1")
  assert((i0.min.0 == 0), "Static constraint violated: i0.min.0 == 0")
  assert((i0.extent.0 == 4096), "Static constraint violated: i0.extent.0 == 4096")
  assert((i0.stride.1 == 4096), "Static constraint violated: i0.stride.1 == 4096")
  assert((i0.min.1 == 0), "Static constraint violated: i0.min.1 == 0")
  assert((i0.extent.1 == 4096), "Static constraint violated: i0.extent.1 == 4096")
  let S_version_0.total_extent.0 = int64(S_version_0.extent.0)
  let S_version_0.total_extent.1.s = int64(S_version_0.extent.1)
  assert((S_version_0.total_extent.0 <= int64(2147483647)), "Total allocation for buffer S_version_0 exceeds 2^31 - 1")
  assert(((S_version_0.total_extent.1.s*int64(S_version_0.stride.1)) <= int64(2147483647)), "Total allocation for buffer S_version_0 exceeds 2^31 - 1")
  assert(((S_version_0.total_extent.1.s*S_version_0.total_extent.0) <= int64(2147483647)), "Product of extents for buffer S_version_0 exceeds 2^31 - 1")
  assert((int64(4096) <= int64(2147483647)), "Total allocation for buffer i0 exceeds 2^31 - 1")
  assert(((int64(4096)*int64(4096)) <= int64(2147483647)), "Total allocation for buffer i0 exceeds 2^31 - 1")
  assert(((int64(4096)*int64(4096)) <= int64(2147483647)), "Product of extents for buffer i0 exceeds 2^31 - 1")
  assert(((0 <= S_version_0.min.1) && ((S_version_0.min.1 + S_version_0.extent.1) <= 4096)), "Bounds given for y in S_version_0 (from %d to %d) don't cover required region (from %d to %d)", 0, 4095, S_version_0.min.1, ((S_version_0.min.1 + S_version_0.extent.1) + -1))
  assert(((0 <= S_version_0.min.0) && ((S_version_0.min.0 + S_version_0.extent.0) <= 4096)), "Bounds given for x in S_version_0 (from %d to %d) don't cover required region (from %d to %d)", 0, 4095, S_version_0.min.0, ((S_version_0.min.0 + S_version_0.extent.0) + -1))
  assert((halide_dev_malloc(S_version_0.buffer) == 0), "Failed to allocate device buffer for S_version_0")
  produce S_version_0 {
    assert((halide_copy_to_dev(i0.buffer) == 0), "Failed to copy buffer i0 to dev.")
    assert((halide_copy_to_dev(S_version_0.buffer) == 0), "Failed to copy buffer S_version_0 to dev.")
    parallel (S_version_0.s0.y.yo.__block_id_y, 0, 128) {
      parallel (S_version_0.s0.x.xo.__block_id_x, 0, 128) {
        allocate __shared[uint8 * 4096]
        parallel (.__thread_id_y, 0, 6) {
          parallel (.__thread_id_x, 0, 32) {
            produce SI_version_0 {
              let SI_version_0.s0.yi.t.base = min((.__thread_id_y*6), 26)
              for (SI_version_0.s0.yi.t, 0, 6) {
                __shared[(.__thread_id_x + ((SI_version_0.s0.yi.t.base + SI_version_0.s0.yi.t)*32))] = i0[(((S_version_0.s0.x.xo.__block_id_x*32) + .__thread_id_x) + (((S_version_0.s0.y.yo.__block_id_y*32) + (SI_version_0.s0.yi.t.base + SI_version_0.s0.yi.t))*4096))]
              }
              halide_gpu_thread_barrier()
            } update SI_version_0 {
              if ((.__thread_id_y < 1)) {
                for (SI_version_0.s1.rxi.x$r, 1, 31) {
                  __shared[(SI_version_0.s1.rxi.x$r + (.__thread_id_x*32))] = (__shared[(SI_version_0.s1.rxi.x$r + (.__thread_id_x*32))] + __shared[((SI_version_0.s1.rxi.x$r + (.__thread_id_x*32)) + -1)])
                }
              }
              halide_gpu_thread_barrier()
              if ((.__thread_id_y < 1)) {
                for (SI_version_0.s2.ryi.x$r, 1, 31) {
                  __shared[(.__thread_id_x + (SI_version_0.s2.ryi.x$r*32))] = (__shared[(.__thread_id_x + (SI_version_0.s2.ryi.x$r*32))] + __shared[((.__thread_id_x + (SI_version_0.s2.ryi.x$r*32)) + -32)])
                }
              }
              halide_gpu_thread_barrier()
            }
            let S_version_0.s0.y.yi.t.base = min((.__thread_id_y*6), 26)
            for (S_version_0.s0.y.yi.t, 0, 6) {
              S_version_0[((((S_version_0.s0.x.xo.__block_id_x*32) + .__thread_id_x) - S_version_0.min.0) + ((((S_version_0.s0.y.yo.__block_id_y*32) + (S_version_0.s0.y.yi.t.base + S_version_0.s0.y.yi.t)) - S_version_0.min.1)*S_version_0.stride.1))] = __shared[((.__thread_id_x + (((S_version_0.s0.y.yi.t.base + S_version_0.s0.y.yi.t) % 32)*32)) + (((S_version_0.s0.y.yi.t.base + S_version_0.s0.y.yi.t)/32)*1024))]
            }
          }
        }
        free __shared
      }
    }
    set_dev_dirty(S_version_0.buffer, uint8(1))
  }
  0
}


Constructing CUDA device codegen
Target triple of initial module: x86_64-unknown-unknown-unknown
Target triple of initial module: x86_64--linux-gnu
Generating llvm bitcode...
Generating llvm bitcode for kernel...
PTX kernel:
//
// Generated by LLVM NVPTX Back-End
//

.version 3.1
.target sm_20
.address_size 64

	// .globl	kernel_S_version_0_s0_y_yo___block_id_y
                                        // @kernel_S_version_0_s0_y_yo___block_id_y
.visible .entry kernel_S_version_0_s0_y_yo___block_id_y(
	.param .u32 kernel_S_version_0_s0_y_yo___block_id_y_param_0,
	.param .u32 kernel_S_version_0_s0_y_yo___block_id_y_param_1,
	.param .u32 kernel_S_version_0_s0_y_yo___block_id_y_param_2,
	.param .u64 kernel_S_version_0_s0_y_yo___block_id_y_param_3,
	.param .u64 kernel_S_version_0_s0_y_yo___block_id_y_param_4
)
{
	.reg .pred 	%p<9>;
	.reg .f32 	%f<19>;
	.reg .s32 	%r<19>;
	.reg .s64 	%rl<132>;

// BB#0:                                // %entry
	ld.param.u64 	%rl21, [kernel_S_version_0_s0_y_yo___block_id_y_param_3];
	ld.param.u32 	%r4, [kernel_S_version_0_s0_y_yo___block_id_y_param_2];
	ld.param.u32 	%r3, [kernel_S_version_0_s0_y_yo___block_id_y_param_1];
	ld.param.u32 	%r2, [kernel_S_version_0_s0_y_yo___block_id_y_param_0];
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r7, %tid.y;
	mov.u32 	%r1, %tid.x;
	ld.param.u64 	%rl22, [kernel_S_version_0_s0_y_yo___block_id_y_param_4];
	mul.lo.s32 	%r8, %r7, 6;
	setp.lt.s32	%p3, %r8, 26;
	cvt.s64.s32	%rl23, %r8;
	selp.b64	%rl1, %rl23, 26, %p3;
	mul.wide.s32 	%rl2, %r5, 32;
	cvt.s64.s32	%rl3, %r1;
	mul.wide.s32 	%rl24, %r6, 32;
	add.s64 	%rl4, %rl3, %rl24;
	add.s64 	%rl25, %rl1, %rl2;
	shl.b64 	%rl26, %rl25, 12;
	add.s64 	%rl27, %rl26, %rl4;
	shl.b64 	%rl28, %rl27, 2;
	add.s64 	%rl29, %rl22, %rl28;
	ld.f32 	%f1, [%rl29];
	shl.b64 	%rl5, %rl1, 5;
	add.s64 	%rl30, %rl5, %rl3;
	shl.b64 	%rl31, %rl30, 2;
	st.shared.f32 	[%rl31], %f1;
	or.b64  	%rl32, %rl1, 1;
	add.s64 	%rl33, %rl32, %rl2;
	shl.b64 	%rl34, %rl33, 12;
	add.s64 	%rl35, %rl34, %rl4;
	shl.b64 	%rl36, %rl35, 2;
	add.s64 	%rl37, %rl22, %rl36;
	ld.f32 	%f2, [%rl37];
	shl.b64 	%rl6, %rl32, 5;
	add.s64 	%rl38, %rl6, %rl3;
	shl.b64 	%rl39, %rl38, 2;
	st.shared.f32 	[%rl39], %f2;
	add.s64 	%rl7, %rl1, 2;
	add.s64 	%rl40, %rl7, %rl2;
	shl.b64 	%rl41, %rl40, 12;
	add.s64 	%rl42, %rl41, %rl4;
	shl.b64 	%rl43, %rl42, 2;
	add.s64 	%rl44, %rl22, %rl43;
	ld.f32 	%f3, [%rl44];
	shl.b64 	%rl8, %rl7, 5;
	add.s64 	%rl45, %rl8, %rl3;
	shl.b64 	%rl46, %rl45, 2;
	st.shared.f32 	[%rl46], %f3;
	add.s64 	%rl9, %rl1, 3;
	add.s64 	%rl47, %rl9, %rl2;
	shl.b64 	%rl48, %rl47, 12;
	add.s64 	%rl49, %rl48, %rl4;
	shl.b64 	%rl50, %rl49, 2;
	add.s64 	%rl51, %rl22, %rl50;
	ld.f32 	%f4, [%rl51];
	shl.b64 	%rl10, %rl9, 5;
	add.s64 	%rl52, %rl10, %rl3;
	shl.b64 	%rl53, %rl52, 2;
	st.shared.f32 	[%rl53], %f4;
	add.s64 	%rl11, %rl1, 4;
	add.s64 	%rl54, %rl11, %rl2;
	shl.b64 	%rl55, %rl54, 12;
	add.s64 	%rl56, %rl55, %rl4;
	shl.b64 	%rl57, %rl56, 2;
	add.s64 	%rl58, %rl22, %rl57;
	ld.f32 	%f5, [%rl58];
	shl.b64 	%rl12, %rl11, 5;
	add.s64 	%rl59, %rl12, %rl3;
	shl.b64 	%rl60, %rl59, 2;
	st.shared.f32 	[%rl60], %f5;
	add.s64 	%rl13, %rl1, 5;
	add.s64 	%rl61, %rl13, %rl2;
	shl.b64 	%rl62, %rl61, 12;
	add.s64 	%rl63, %rl62, %rl4;
	shl.b64 	%rl64, %rl63, 2;
	add.s64 	%rl65, %rl22, %rl64;
	ld.f32 	%f6, [%rl65];
	shl.b64 	%rl14, %rl13, 5;
	add.s64 	%rl66, %rl14, %rl3;
	shl.b64 	%rl67, %rl66, 2;
	st.shared.f32 	[%rl67], %f6;
	bar.sync 	0;
	setp.gt.s32	%p4, %r7, 0;
	@%p4 bra 	BB0_1;
	bra.uni 	BB0_2;
BB0_1:
	mov.pred 	%p8, 0;
	bra.uni 	BB0_3;
BB0_2:                                  // %for SI_version_0.s1.rxi.x$r.preheader
	mul.wide.s32 	%rl15, %r1, 128;
	mov.u64 	%rl130, 0;
BB0_5:                                  // %for SI_version_0.s1.rxi.x$r
                                        // =>This Inner Loop Header: Depth=1
	add.s64 	%rl69, %rl15, %rl130;
	ld.shared.f32 	%f7, [%rl69];
	ld.shared.f32 	%f8, [%rl69+4];
	add.f32 	%f9, %f7, %f8;
	st.shared.f32 	[%rl69+4], %f9;
	add.s64 	%rl130, %rl130, 4;
	setp.eq.s64	%p6, %rl130, 124;
	@%p6 bra 	BB0_6;
	bra.uni 	BB0_5;
BB0_6:
	mov.pred 	%p8, -1;
BB0_3:                                  // %after_bb
	bar.sync 	0;
	@!%p8 bra 	BB0_7;
	bra.uni 	BB0_4;
BB0_4:                                  // %for SI_version_0.s2.ryi.x$r.preheader
	mul.wide.s32 	%rl16, %r1, 4;
	mov.u64 	%rl131, 0;
BB0_8:                                  // %for SI_version_0.s2.ryi.x$r
                                        // =>This Inner Loop Header: Depth=1
	add.s64 	%rl71, %rl16, %rl131;
	ld.shared.f32 	%f10, [%rl71];
	ld.shared.f32 	%f11, [%rl71+128];
	add.f32 	%f12, %f10, %f11;
	st.shared.f32 	[%rl71+128], %f12;
	add.s64 	%rl131, %rl131, 128;
	setp.eq.s64	%p7, %rl131, 3968;
	@%p7 bra 	BB0_7;
	bra.uni 	BB0_8;
BB0_7:                                  // %after_bb3
	bar.sync 	0;
	cvt.s64.s32	%rl72, %r4;
	cvt.s64.s32	%rl73, %r3;
	sub.s64 	%rl74, %rl2, %rl73;
	add.s64 	%rl75, %rl74, %rl1;
	cvt.s64.s32	%rl76, %r2;
	sub.s64 	%rl77, %rl4, %rl76;
	cvt.u32.u64	%r9, %rl1;
	shr.s32 	%r10, %r9, 5;
	mul.wide.s32 	%rl78, %r10, 1024;
	and.b64  	%rl79, %rl5, 960;
	add.s64 	%rl80, %rl79, %rl3;
	add.s64 	%rl81, %rl80, %rl78;
	shl.b64 	%rl82, %rl81, 2;
	ld.shared.f32 	%f13, [%rl82];
	mad.lo.s64 	%rl83, %rl75, %rl72, %rl77;
	shl.b64 	%rl84, %rl83, 2;
	add.s64 	%rl85, %rl21, %rl84;
	st.f32 	[%rl85], %f13;
	and.b64  	%rl86, %rl6, 992;
	add.s64 	%rl87, %rl86, %rl3;
	add.s64 	%rl88, %rl87, %rl78;
	shl.b64 	%rl89, %rl88, 2;
	ld.shared.f32 	%f14, [%rl89];
	add.s64 	%rl90, %rl75, 1;
	mad.lo.s64 	%rl91, %rl90, %rl72, %rl77;
	shl.b64 	%rl92, %rl91, 2;
	add.s64 	%rl93, %rl21, %rl92;
	st.f32 	[%rl93], %f14;
	cvt.u32.u64	%r11, %rl7;
	shr.s32 	%r12, %r11, 5;
	mul.wide.s32 	%rl94, %r12, 1024;
	and.b64  	%rl95, %rl8, 960;
	add.s64 	%rl96, %rl95, %rl3;
	add.s64 	%rl97, %rl96, %rl94;
	shl.b64 	%rl98, %rl97, 2;
	ld.shared.f32 	%f15, [%rl98];
	add.s64 	%rl99, %rl75, 2;
	mad.lo.s64 	%rl100, %rl99, %rl72, %rl77;
	shl.b64 	%rl101, %rl100, 2;
	add.s64 	%rl102, %rl21, %rl101;
	st.f32 	[%rl102], %f15;
	cvt.u32.u64	%r13, %rl9;
	shr.s32 	%r14, %r13, 5;
	mul.wide.s32 	%rl103, %r14, 1024;
	and.b64  	%rl104, %rl10, 992;
	add.s64 	%rl105, %rl104, %rl3;
	add.s64 	%rl106, %rl105, %rl103;
	shl.b64 	%rl107, %rl106, 2;
	ld.shared.f32 	%f16, [%rl107];
	add.s64 	%rl108, %rl75, 3;
	mad.lo.s64 	%rl109, %rl108, %rl72, %rl77;
	shl.b64 	%rl110, %rl109, 2;
	add.s64 	%rl111, %rl21, %rl110;
	st.f32 	[%rl111], %f16;
	cvt.u32.u64	%r15, %rl11;
	shr.s32 	%r16, %r15, 5;
	mul.wide.s32 	%rl112, %r16, 1024;
	and.b64  	%rl113, %rl12, 960;
	add.s64 	%rl114, %rl113, %rl3;
	add.s64 	%rl115, %rl114, %rl112;
	shl.b64 	%rl116, %rl115, 2;
	ld.shared.f32 	%f17, [%rl116];
	add.s64 	%rl117, %rl75, 4;
	mad.lo.s64 	%rl118, %rl117, %rl72, %rl77;
	shl.b64 	%rl119, %rl118, 2;
	add.s64 	%rl120, %rl21, %rl119;
	st.f32 	[%rl120], %f17;
	cvt.u32.u64	%r17, %rl13;
	shr.s32 	%r18, %r17, 5;
	mul.wide.s32 	%rl121, %r18, 1024;
	and.b64  	%rl122, %rl14, 992;
	add.s64 	%rl123, %rl122, %rl3;
	add.s64 	%rl124, %rl123, %rl121;
	shl.b64 	%rl125, %rl124, 2;
	ld.shared.f32 	%f18, [%rl125];
	add.s64 	%rl126, %rl75, 5;
	mad.lo.s64 	%rl127, %rl126, %rl72, %rl77;
	shl.b64 	%rl128, %rl127, 2;
	add.s64 	%rl129, %rl21, %rl128;
	st.f32 	[%rl129], %f18;
	ret;
}

